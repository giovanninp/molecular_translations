{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install timm","metadata":{"execution":{"iopub.status.busy":"2022-06-03T13:12:39.533915Z","iopub.execute_input":"2022-06-03T13:12:39.538499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from io import BytesIO\nfrom IPython.display import HTML\n\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n\nimport os\nimport gc\nimport re\nimport math\nimport time\nimport random\nimport shutil\nimport pickle\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport Levenshtein\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, Blur\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport re\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom subprocess import Popen, PIPE\n\nfrom logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DIRECTIORES\n\nBASE_DIR = \"../input/bms-molecular-translation\"\nTRAIN_DIR = f\"{BASE_DIR}/train\"\nTEST_DIR = f\"{BASE_DIR}/test\"\nOUTPUT_DIR = './'\n\nDIR_TYPES = {\n    \"train\": TRAIN_DIR,\n    \"test\": TEST_DIR\n}\n\n# PATH UTILS\n\nget_path = lambda items_ids: \"/\".join(list(map(lambda id: f\"{id}\" ,items_ids)))\nget_file_path = lambda file_type, file_id, suffix = \"\": f\"{DIR_TYPES[file_type]}/{get_path(file_id)}{suffix}\"\n\n\"\"\"Get a image path\n\nparams: \n    image_id: string\n        image id to be retrieved\n    image_type: 'train' | 'test'\n\"\"\"\nget_image_path = lambda image_id, file_type = 'train': f\"{get_file_path(file_type, [image_id[0],image_id[1],image_id[2], image_id], '.png')}\"\nget_image_path_test = lambda image_id, file_type = 'test': f\"{get_file_path(file_type, [image_id[0],image_id[1],image_id[2], image_id], '.png')}\"\n\ndef has_file(filename):\n    \"\"\"Verify if a file exists\n    \n    params:\n    ------\n        filename: string\n            the filename to verify existence\n    returns:\n    -------\n        Returns a boolean wheres concludes the file existence\n    \"\"\"\n    process = Popen('ls', stdout=PIPE)\n    output, error = process.communicate()\n    process.kill()\n    if error: return False;\n    return filename in f\"{output}\".split(\"\\\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMAGE PREVIEW UTILS\n\npd.set_option('display.max_colwidth', None)\n\ndef get_thumbnail(path):\n    image = Image.open(path)\n    image.thumbnail((200, 200), Image.LANCZOS)\n    return image\n\ndef image_base64(im):\n    if isinstance(im, str):\n        im = get_thumbnail(im)\n    with BytesIO() as buffer:\n        im.save(buffer, 'jpeg')\n        return base64.b64encode(buffer.getvalue()).decode()\n\ndef image_formatter(im):\n    return f'<img src=\"data:image/jpeg;base64,{image_base64(im)}\">'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TOKENIZER CLASS\n\nclass Tokenizer(object):\n    def __init__(self):\n        self.stoi = {}\n        self.itos = {}\n    \n    def __len__(self):\n        return len(self.stoi)\n\n    def fit_on_texts(self, texts = []):\n        \"\"\" Fill stoi and itos with vocabulary\n\n        Parameters\n        ----------\n        texts: list of string\n            list of texts used to be tokenized, where fill the stoi(string to index) and \n            itos(index to string) sets. \n        \"\"\"\n\n        vocab = map(lambda text: text.split(' '), texts)\n        sorted_vocab = sorted([item for sublist in vocab for item in sublist if item != ''])\n        final_vocab = {*sorted_vocab, '<sos>', '<eos>', '<pad>'}\n\n\n        for idx, string in enumerate(final_vocab): \n            self.stoi[string] = idx\n            self.itos[idx] = string\n        \n    def text_to_sequence(self, text = \"\"):\n        \"\"\" Parse the received text to a vector of indexes related to each token.\n\n        Parameters\n        ----------\n        text: string\n            Text to be parsed to string.\n\n        Returns\n        -------\n            Returns a array of integers.\n        \"\"\"\n        sequence = []\n        sequence.append(self.stoi['<sos>'])\n        for string in text.split(' '): sequence.append(self.stoi[string])\n        sequence.append(self.stoi['<eos>'])\n        return sequence\n    \n    def texts_to_sequences(self, texts = []):\n        \"\"\" Parse a batch of texts into a array of sequences\n\n        Parameters\n        ----------\n        texts: [string]\n            A array of texts.\n\n        Returns\n        -------\n            Returns a array of array of integers.\n        \n        \"\"\"\n        return list(map(self.text_to_sequence, texts))\n    \n    def sequence_to_text(self, sequence = []):\n        \"\"\" Parse a sequence into a text\n\n        Parameters\n        ----------\n        sequence: [integer]\n            A sequence where will be parsed by accessing the giving numbers to retrieve\n            the related texts from itos(itos).\n\n        Returns\n        -------\n            Returns a string\n        \"\"\"\n\n        return \" \".join(list(map(\n            lambda idx: self.itos[idx],\n            sequence\n        )))\n\n    def sequences_to_texts(self, sequences = []):\n        \"\"\" Parse a batch of sequence into a array related texts\n\n        Parameters\n        ----------\n        sequences: [[integer]]\n            A array of sequences where will be parsed by accessing the giving numbers to\n            retrieve the related texts from itos(itos).\n\n        Returns\n        -------\n            Array of strings\n        \"\"\"\n        \n        return list(map(self.sequence_to_text, sequences))\n    \n    def predict_caption(self, sequence = []):\n        \"\"\" Parse a sequence into a text based and limited by eos(end of sequence) or \n            pad tokens\n        \n        Parameters\n        ----------\n        sequence: [integer]\n            A sequence where will be parsed by accessing the giving numbers to retrieve\n            the related texts from itos(itos).\n\n        Returns\n        -------\n            Returns a text\n        \n        \"\"\"\n        caption = []\n        for idx in sequence:\n            curr_string = self.itos[idx]\n            if curr_string in [\"<pad>\", \"<eos>\"]: break\n            caption.append(curr_string)\n        return \" \".join(caption)\n    \n    def predict_captions(self, sequences = []):\n        \"\"\" Parse a batch of sequences into a text based and limited by eos(end of sequence) or \n            pad tokens\n        \n        Parameters\n        ----------\n        sequence: [[integer]]\n            A array of sequences where will be parsed by accessing the giving numbers to \n            retrieve the related texts from itos(itos).\n\n        Returns\n        -------\n            Returns a array of texts\n        \n        \"\"\"\n        return list(map(self.predict_caption, sequences))\n\ndef run_tokenizer_test(): # TODO Remove when ended\n    error = None\n\n    test_tokenizer = Tokenizer()\n    test_texts = [\n        'C 13 H 20 O S /c 1 - 9 ( 2 ) 8 - 15 - 13 - 6 - 5 - 10 ( 3 ) 7 - 12 ( 13 ) 11 ( 4 ) 14 /h 5 - 7 , 9 , 11 , 14 H , 8 H 2 , 1 - 4 H 3',\n        'C 21 H 30 O 4 /c 1 - 12 ( 22 ) 25 - 14 - 6 - 8 - 20 ( 2 ) 13 ( 10 - 14 ) 11 - 17 ( 23 ) 19 - 15 - 4 - 5 - 18 ( 24 ) 21 ( 15 , 3 ) 9 - 7 - 16 ( 19 ) 20 /h 13 - 16 , 19 H , 4 - 11 H 2 , 1 - 3 H 3 /t 13 - , 14 + , 15 + , 16 - , 19 - , 20 + , 21 + /m 1 /s 1',\n        'C 24 H 23 N 5 O 4 /c 1 - 14 - 13 - 15 ( 7 - 8 - 17 ( 14 ) 28 - 12 - 10 - 20 ( 28 ) 30 ) 27 - 11 - 9 - 16 - 21 ( 23 ( 25 ) 31 ) 26 - 29 ( 22 ( 16 ) 24 ( 27 ) 32 ) 18 - 5 - 3 - 4 - 6 - 19 ( 18 ) 33 - 2 /h 3 - 8 , 13 H , 9 - 12 H 2 , 1 - 2 H 3 , ( H 2 , 25 , 31 )'\n    ]\n    test_tokenizer.fit_on_texts(test_texts)\n\n    expected = ['(', ')', '+', ',', '-', '/c', '/h', '/m', '/s', '/t', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '4', '5', '6', '7', '8', '9', 'C', 'H', 'N', 'O', 'S', '<sos>', '<eos>', '<pad>']\n    rest = [item for item in expected if item not in list(test_tokenizer.stoi)]\n\n    if len(rest): error = 'Mismatching stoi'\n\n    if error != None: raise Exception(error)\n\n\n    \n\n\nrun_tokenizer_test()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATASET READING\n\ntokenizer = '../input/inchipreprocess2/tokenizer2.pth'\nTRAIN_PICKLE_FILENAME = '../input/inchipreprocess2/train2.pkl'\n\nclass PreProcess:\n    def __init__(self):\n        print(\"INITIALIZED\\n\")\n        print(\"LOADING TOKENIZER...\\n\")\n        self.tokenizer = Tokenizer()\n        self.tokenizer = torch.load('../input/inchipreprocess2/tokenizer2.pth')\n\n        print(\"LOADING TRAIN PICKLE...\\n\")\n        self.train = pd.read_pickle('../input/inchipreprocess2/train2.pkl')\n        self.train['file_path'] = self.train['image_id'].apply(get_image_path)\n        self.test = pd.read_csv('../input/bms-molecular-translation/sample_submission.csv')\n        self.test['file_path'] = self.test['image_id'].apply(get_image_path_test)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_process = PreProcess()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pre_process.train.columns)\nprint(pre_process.test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pre_process.train.describe())\nprint(\"\\n\")\nprint(pre_process.test.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_process.train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_process.test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pre_process.train.dtypes)\nprint(\"\\n\")\nprint(pre_process.test.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ITEM EXAMPLE\n\nprint(pre_process.train.loc[0])\nprint(\"\\n\")\nprint(pre_process.test.loc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAIN IMAGE SAMPLE\n\nget_thumbnail(pre_process.train['file_path'].values[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST IMAGE SAMPLE\n\nget_thumbnail(get_image_path(pre_process.test.loc[0].image_id, 'test'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_form(form):\n    string = ''\n    for i in re.findall(r\"[A-Z][^A-Z]*\", form):\n        elem = re.match(r\"\\D+\", i).group()\n        num = i.replace(elem, \"\")\n        if num == \"\":\n            string += f\"{elem} \"\n        else:\n            string += f\"{elem} {str(num)} \"\n    return string.rstrip(' ')\n\ndef split_form2(form):\n    string = ''\n    for i in re.findall(r\"[a-z][^a-z]*\", form):\n        elem = i[0]\n        num = i.replace(elem, \"\").replace('/', \"\")\n        num_string = ''\n        for j in re.findall(r\"[0-9]+[^0-9]*\", num):\n            num_list = list(re.findall(r'\\d+', j))\n            assert len(num_list) == 1, f\"len(num_list) != 1\"\n            _num = num_list[0]\n            if j == _num:\n                num_string += f\"{_num} \"\n            else:\n                extra = j.replace(_num, \"\")\n                num_string += f\"{_num} {' '.join(list(extra))} \"\n        string += f\"/{elem} {num_string}\"\n    return string.rstrip(' ')\n\nsplit_form_compound = lambda inChI: split_form2(split_form(inChI.split('=')[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"splitByNumber = r'(\\d+)'\nsplitText = r'/[a-zA-Z][a-zA-Z]'\n\ndef parse_formula(inChI = ''):\n    result = map(lambda string: re.split(splitByNumber,string) ,re.split(splitText, inChI))\n    return np.unique([item for sublist in result for item in sublist if item != ''][1:])\n\ndef parse_formulas(formulas = []):\n    result = map(parse_formula, formulas)\n    return np.unique([item for sublist in result for item in sublist if item != ''][1:])\n\nprint(\" \".join(parse_formula('InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(10-14)11-17(23)19-15-4-5-18(24)21(15,3)9-7-16(19)20/h13-16,19H,4-11H2,1-3H3/t13-,14+,15+,16-,19-,20+,21+/m1/s1')))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up CFG\n\nclass CFG:\n    debug=True\n    max_len=275\n    print_freq=1000\n    num_workers=4\n    model_name='resnet34'\n    size=224\n    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    epochs=1\n    #factor=0.2 # ReduceLROnPlateau\n    #patience=4 # ReduceLROnPlateau\n    #eps=1e-6 # ReduceLROnPlateau\n    T_max=4 # CosineAnnealingLR\n    #T_0=4 # CosineAnnealingWarmRestarts\n    encoder_lr=1e-4\n    decoder_lr=4e-4\n    min_lr=1e-6\n    batch_size=8\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=5\n    attention_dim=256\n    embed_dim=256\n    decoder_dim=512\n    dropout=0.5\n    seed=42\n    n_fold=2\n    trn_fold=[0] \n    train=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CFG.debug:\n    CFG.epochs = 1\n    pre_process.train = pre_process.train.sample(n=30000, random_state=CFG.seed).reset_index(drop=True)\n    pre_process.test = pre_process.test.sample(n=30000, random_state=CFG.seed).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_process.test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SEED TORCH\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SCORING UTILS\n\ndef get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CV SPLIT\nif(CFG.train):\n    folds = pre_process.train.copy()\n    Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['InChI_length'])):\n        folds.loc[val_index, 'fold'] = int(n)\n    folds['fold'] = folds['fold'].astype(int)\n    print(folds.groupby(['fold']).size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATASET\nclass TrainDataset(Dataset):\n    def __init__(self, df, tokenizer, transform=None):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.file_paths = df['file_path'].values\n        self.labels = df['InChI_text'].values\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        label = self.labels[idx]\n        label = self.tokenizer.text_to_sequence(label)\n        label_length = len(label)\n        print(label,label_length)\n        label_length = torch.LongTensor([label_length])\n        return image, torch.LongTensor(label), label_length\n    \nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        super().__init__()\n        self.df = df\n        self.file_paths = df['file_path'].values\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bms_collate(batch):\n    imgs, labels, label_lengths = [], [], []\n    for data_point in batch:\n        imgs.append(data_point[0])\n        labels.append(data_point[1])\n        label_lengths.append(data_point[2])\n    labels = pad_sequence(labels, batch_first=True, padding_value=pre_process.tokenizer.stoi[\"<pad>\"])\n    return torch.stack(imgs), labels, torch.stack(label_lengths).reshape(-1, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_process.tokenizer.sequence_to_text([1,2,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Showing train dataset\ntrain_dataset = TrainDataset(pre_process.train, pre_process.tokenizer, transform=get_transforms(data='train'))\n\nfor i in range(1):\n    train_dataset[i]\n    image, label, label_length = train_dataset[i]\n    text = pre_process.tokenizer.sequence_to_text(label.numpy())\n    plt.imshow(image.transpose(0, 1).transpose(1, 2))\n    plt.title(f'label: {label}  text: {text}  label_length: {label_length}')\n    plt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=True):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.n_features = self.cnn.fc.in_features\n        self.cnn.global_pool = nn.Identity()\n        self.cnn.fc = nn.Identity()\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.cnn(x)\n        features = features.permute(0, 2, 3, 1)\n        return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n \n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n  \n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim) \n        self.full_att = nn.Linear(attention_dim, 1)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  \n        att2 = self.decoder_att(decoder_hidden)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n        alpha = self.softmax(att)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=512, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.device = device\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim) \n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True) \n        self.init_h = nn.Linear(encoder_dim, decoder_dim) \n        self.init_c = nn.Linear(encoder_dim, decoder_dim) \n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  \n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size) \n        self.init_weights()  \n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out) \n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim) \n        num_pixels = encoder_out.size(1)\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        embeddings = self.embedding(encoded_captions) \n        h, c = self.init_hidden_state(encoder_out) \n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  \n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t])) \n            preds = self.fc(self.dropout(h)) \n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, tokenizer):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n        num_pixels = encoder_out.size(1)\n        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * tokenizer.stoi[\"<sos>\"]\n        embeddings = self.embedding(start_tockens)\n        h, c = self.init_hidden_state(encoder_out)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n        for t in range(decode_lengths):\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  \n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                (h, c)) \n            preds = self.fc(self.dropout(h))  \n            predictions[:, t, :] = preds\n            if np.argmax(preds.detach().cpu().numpy()) == tokenizer.stoi[\"<eos>\"]:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(train_loader, encoder, decoder, criterion, \n             encoder_optimizer, decoder_optimizer, epoch,\n             encoder_scheduler, decoder_scheduler, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    encoder.train()\n    decoder.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels, label_lengths) in enumerate(train_loader):\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        batch_size = images.size(0)\n        features = encoder(images)\n        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n        targets = caps_sorted[:, 1:]\n        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        loss = criterion(predictions, targets)\n        losses.update(loss.item(), batch_size)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        loss.backward()\n        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            global_step += 1\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n                  #'Encoder LR: {encoder_lr:.6f}  '\n                  #'Decoder LR: {decoder_lr:.6f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)/len(train_loader)),\n                   encoder_grad_norm=encoder_grad_norm,\n                   decoder_grad_norm=decoder_grad_norm,\n                   #encoder_lr=encoder_scheduler.get_lr()[0],\n                   #decoder_lr=decoder_scheduler.get_lr()[0],\n                   ))\n    return losses.avg\n\n\ndef valid_fn(valid_loader, encoder, decoder, tokenizer, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    encoder.eval()\n    decoder.eval()\n    text_preds = []\n    start = end = time.time()\n    for step, (images) in enumerate(valid_loader):\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        batch_size = images.size(0)\n        with torch.no_grad():\n            features = encoder(images)\n            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        _text_preds = tokenizer.predict_captions(predicted_sequence)\n        text_preds.append(_text_preds)\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  .format(\n                   step, len(valid_loader), batch_time=batch_time,\n                   data_time=data_time,\n                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n                   ))\n    text_preds = np.concatenate(text_preds)\n    return text_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(folds, fold):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds['InChI'].values\n\n    train_dataset = TrainDataset(train_folds, pre_process.tokenizer, transform=get_transforms(data='train'))\n    valid_dataset = TestDataset(valid_folds, transform=get_transforms(data='valid'))\n\n    train_loader = DataLoader(train_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=True, \n                              num_workers=CFG.num_workers, \n                              pin_memory=True,\n                              drop_last=True, \n                              collate_fn=bms_collate)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG.batch_size, \n                              shuffle=False, \n                              num_workers=CFG.num_workers,\n                              pin_memory=True, \n                              drop_last=False)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    encoder = Encoder(CFG.model_name, pretrained=True)\n    encoder.to(device)\n    encoder_optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    encoder_scheduler = get_scheduler(encoder_optimizer)\n    \n    decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n                                   embed_dim=CFG.embed_dim,\n                                   decoder_dim=CFG.decoder_dim,\n                                   vocab_size=len(pre_process.tokenizer),\n                                   dropout=CFG.dropout,\n                                   device=device)\n    decoder.to(device)\n    decoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    decoder_scheduler = get_scheduler(decoder_optimizer)\n\n    criterion = nn.CrossEntropyLoss(ignore_index=pre_process.tokenizer.stoi[\"<pad>\"])\n\n    best_score = np.inf\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n                            encoder_optimizer, decoder_optimizer, epoch, \n                            encoder_scheduler, decoder_scheduler, device)\n\n        # eval\n        text_preds = valid_fn(valid_loader, encoder, decoder, pre_process.tokenizer, criterion, device)\n        text_preds = [f\"InChI=1S/{text}\" for text in text_preds]\n        LOGGER.info(f\"labels: {valid_labels[:5]}\")\n        LOGGER.info(f\"preds: {text_preds[:5]}\")\n        \n        # scoring\n        score = get_score(valid_labels, text_preds)\n        \n        if isinstance(encoder_scheduler, ReduceLROnPlateau):\n            encoder_scheduler.step(score)\n        elif isinstance(encoder_scheduler, CosineAnnealingLR):\n            encoder_scheduler.step()\n        elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n            encoder_scheduler.step()\n            \n        if isinstance(decoder_scheduler, ReduceLROnPlateau):\n            decoder_scheduler.step(score)\n        elif isinstance(decoder_scheduler, CosineAnnealingLR):\n            decoder_scheduler.step()\n        elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n            decoder_scheduler.step()\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        if score < best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'encoder': encoder.state_dict(), \n                        'encoder_optimizer': encoder_optimizer.state_dict(), \n                        'encoder_scheduler': encoder_scheduler.state_dict(), \n                        'decoder': decoder.state_dict(), \n                        'decoder_optimizer': decoder_optimizer.state_dict(), \n                        'decoder_scheduler': decoder_scheduler.state_dict(), \n                        'text_preds': text_preds,\n                       },\n                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#When CFG.train is true to training the train dataset\n\ndef main():\n\n    \"\"\"\n    Prepare: 1.train  2.folds\n    \"\"\"\n\n    if CFG.train:\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                print(fold)\n                train_loop(folds, fold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def inference(test_loader, encoder, decoder, tokenizer, device):\n#    encoder.eval()\n#    decoder.eval()\n#    text_preds = []\n#    tk0 = tqdm(test_loader, total= 1616107)\n#    for images in tk0:\n#        images = images.to(device)\n#        with torch.no_grad():\n#            features = encoder(images)\n#            predictions = decoder.predict(features, CFG.max_len, tokenizer)\n#        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n#        _text_preds = tokenizer.predict_captions(predicted_sequence)\n#        text_preds.append(_text_preds)\n#    text_preds = np.concatenate(text_preds)\n#    return text_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encoder = Encoder(CFG.model_name, pretrained= True)\n#encoder.to(device)\n\n#decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n#                               embed_dim=CFG.embed_dim,\n#                               decoder_dim=CFG.decoder_dim,\n#                               vocab_size=len(pre_process.tokenizer),\n#                               dropout=CFG.dropout,\n#                               device=device)\n#decoder.to(device)\n\n#test_dataset = TestDataset(pre_process.test, transform=get_transforms(data='valid'))\n#test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=CFG.num_workers)\n#predictions = inference(test_loader, encoder, decoder, pre_process.tokenizer, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission\n#pre_process.test['InChI'] = [f\"InChI=1S/{text}\" for text in predictions]\n#pre_process.test[['image_id', 'InChI']].to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pre_process.test[['image_id', 'InChI']].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}